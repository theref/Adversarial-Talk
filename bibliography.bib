@article{Finlayson_Chung_Kohane_Beam_2018, title={Adversarial Attacks Against Medical Deep Learning Systems}, url={http://arxiv.org/abs/1804.05296}, abstractNote={The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we argue that the field of medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud, we extend adversarial attacks to three popular medical imaging tasks, and we provide concrete examples of how and why such attacks could be realistically carried out. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. We urge caution in deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.}, note={arXiv: 1804.05296}, journal={arXiv:1804.05296 [cs, stat]}, author={Finlayson, Samuel G. and Chung, Hyung Won and Kohane, Isaac S. and Beam, Andrew L.}, year={2018}, month={04}}
@misc{Commissioner, title={Press Announcements - FDA permits marketing of artificial intelligence-based device to detect certain diabetes-related eye problems}, url={https://www.fda.gov/NewsEvents/Newsroom/PressAnnouncements/ucm604357.htm}, abstractNote={FDA permits marketing of first medical device to use artificial intelligence to detect greater than a mild level of diabetic retinopathy in the eye of adults who have diabetes.}, author={Commissioner, Office of the}}
@article{Litjens_Kooi_Bejnordi_Setio_Ciompi_Ghafoorian_van_der_Laak_van_Ginneken_Sánchez_2017, title={A Survey on Deep Learning in Medical Image Analysis}, volume={42}, ISSN={13618415}, DOI={10.1016/j.media.2017.07.005}, abstractNote={Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed.}, note={arXiv: 1702.05747}, journal={Medical Image Analysis}, author={Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A. W. M. and van Ginneken, Bram and Sánchez, Clara I.}, year={2017}, month={12}, pages={60-88}}
@article{Ilyas_Engstrom_Athalye_Lin_2018, title={Black-box Adversarial Attacks with Limited Queries and Information}, url={http://arxiv.org/abs/1804.08598}, abstractNote={Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.}, note={arXiv: 1804.08598}, journal={arXiv:1804.08598 [cs, stat]}, author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy}, year={2018}, month={04}}
@article{Athalye_Engstrom_Ilyas_Kwok_2017, title={Synthesizing Robust Adversarial Examples}, url={http://arxiv.org/abs/1707.07397}, abstractNote={Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.}, note={arXiv: 1707.07397}, journal={arXiv:1707.07397 [cs]}, author={Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin}, year={2017}, month={07}}
@article{Eykholt_Evtimov_Fernandes_Li_Rahmati_Xiao_Prakash_Kohno_Song_2017, title={Robust Physical-World Attacks on Deep Learning Models}, url={http://arxiv.org/abs/1707.08945}, abstractNote={Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100\% of the images obtained in lab settings, and in 84.8\%of the captured video frames obtained on a moving vehicle(field test) for the target classifier.}, note={arXiv: 1707.08945}, journal={arXiv:1707.08945 [cs]}, author={Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn}, year={2017}, month={07}}
@article{Jain_Nundy_Abbasi_2014, title={Corruption: medicine’s dirty open secret}, volume={348}, ISSN={1756-1833}, DOI={10.1136/bmj.g4184}, abstractNote={<p>Doctors must fight back against kickbacks</p>}, journal={BMJ}, author={Jain, Anita and Nundy, Samiran and Abbasi, Kamran}, year={2014}, month={06}}
@article{Pien_Fischman_Thrall_Sorensen_2005, title={Using imaging biomarkers to accelerate drug development and clinical trials}, volume={10}, ISSN={1359-6446}, DOI={10.1016/S1359-6446(04)03334-3}, abstractNote={There is increasing evidence that human medical imaging can help answer key questions that arise during the drug development process. Imaging modalities such as magnetic resonance imaging, computed tomography and positron emission tomography can offer significant insights into the bioactivity, pharmacokinetics and dosing of drugs, in addition to supporting registration applications. In this review, examples from oncology, neurology, psychiatry, infectious diseases and inflammatory diseases are used to illustrate the role imaging can play. We conclude with some remarks concerning new developments that will be required to significantly advance the field of pharmaco-imaging.}, number={4}, journal={Drug Discovery Today}, author={Pien, Homer H. and Fischman, Alan J. and Thrall, James H. and Sorensen, A. Gregory}, year={2005}, month={02}, pages={259–266}}
@article{Njeh_2008, title={Tumor delineation: The weakest link in the search for accuracy in radiotherapy}, volume={33}, ISSN={0971-6203}, DOI={10.4103/0971-6203.44472}, abstractNote={Radiotherapy is one of the most effective modalities for the treatment of cancer. However, there is a high degree of uncertainty associated with the target volume of most cancer sites. The sources of these uncertainties include, but are not limited to, the motion of the target, patient setup errors, patient movements, and the delineation of the target volume. Recently, many imaging techniques have been introduced to track the motion of tumors. The treatment delivery using these techniques is collectively called image-guided radiation therapy (IGRT). Ultimately, IGRT is only as good as the accuracy with which the target is known. There are reports of interobserver variability in tumor delineation across anatomical sites, but the widest ranges of variations have been reported for the delineation of head and neck tumors as well as esophageal and lung carcinomas. Significant interobserver variability in target delineation can be attributed to many factors including the impact of imaging and the influence of the observer (specialty, training, and personal bias). The visibility of the target can be greatly improved with the use of multimodality imaging by co-registration of CT with a second modality such as magnetic resonance imaging (MRI) and/or positron emission tomography. Also, continuous education, training, and cross-collaboration of the radiation oncologist with other specialties can reduce the degree of variability in tumor delineation.}, number={4}, journal={Journal of Medical Physics}, author={Njeh, C. F.}, year={2008}, month={10}, pages={136}}
@article{Madry_Makelov_Schmidt_Tsipras_Vladu_2017, title={Towards Deep Learning Models Resistant to Adversarial Attacks}, url={http://arxiv.org/abs/1706.06083}, abstractNote={Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.}, note={arXiv: 1706.06083}, journal={arXiv:1706.06083 [cs, stat]}, author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian}, year={2017}, month={06}}
@article{Goodfellow_Shlens_Szegedy_2014, title={Explaining and Harnessing Adversarial Examples}, url={http://arxiv.org/abs/1412.6572}, abstractNote={Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.}, note={arXiv: 1412.6572}, journal={arXiv:1412.6572 [cs, stat]}, author={Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian}, year={2014}, month={12}}
@article{Ilyas_Engstrom_Athalye_Lin_2018, title={Black-box Adversarial Attacks with Limited Queries and Information}, url={http://arxiv.org/abs/1804.08598}, abstractNote={Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.}, note={arXiv: 1804.08598}, journal={arXiv:1804.08598 [cs, stat]}, author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy}, year={2018}, month={04}}
@article{Brown_Mané_Roy_Abadi_Gilmer_2017, title={Adversarial Patch}, url={http://arxiv.org/abs/1712.09665}, abstractNote={We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classifiers; even when the patches are small, they cause the classifiers to ignore the other items in the scene and report a chosen target class. To reproduce the results from the paper, our code is available at https://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_patch}, note={arXiv: 1712.09665}, journal={arXiv:1712.09665 [cs]}, author={Brown, Tom B. and Mané, Dandelion and Roy, Aurko and Abadi, Martín and Gilmer, Justin}, year={2017}, month={12}}
